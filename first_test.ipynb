{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy_sentiws import spaCySentiWS\n",
    "import os\n",
    "nlp = spacy.load('de')\n",
    "sentiws = spaCySentiWS(sentiws_path=os.path.join(os.getcwd(), 'aux'))\n",
    "nlp.add_pipe(sentiws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(os.getcwd(), \"input\")\n",
    "with open(os.path.join(input_path, \"positive_example5.txt\"), \"r\") as f:\n",
    "     text = f.read()  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, substitution_dict):\n",
    "    for key, value in sub_dict.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary with patterns to substitute for cleaning purposes\n",
    "sub_dict = {\n",
    "    \"\\n\\n\": \"\\n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text, sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"flüchtlinge\", \"flüchtling\", \"geflüchtete\", \"geflüchteter\", \"geflüchtete\", \"asylbewerber\", \"asylsuchender\", \"asylant\", \"migrant\", \"migranten\", \"migrantin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: It is probably good to give each article an ID in my collection so that I don't have to save the whole doc with the score.\n",
    "def score_articles(doc):\n",
    "    results = []\n",
    "    sentiments = [token._.sentiws for token in doc if token._.sentiws and token.lower_]\n",
    "    if len(sentiments) > 0:\n",
    "        score = sum(sentiments) / len(sentiments)\n",
    "        results.append((doc.text[:100], score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(doc, targets):\n",
    "    \"\"\"Score sentences that contain any of the target patterns. Returns a list of tuples\"\"\"\n",
    "    compiled_targets = [re.compile(target) for target in targets]\n",
    "    results = []\n",
    "    for sentence in doc.sents:\n",
    "        if any(target.search(sentence.lower_) for target in compiled_targets):\n",
    "            # I exclude \"Flüchtlinge\" and synonyms because they are scored as negative sentiment in the model\n",
    "            sentiments = [token._.sentiws for token in sentence if token._.sentiws and token.lower_ not in targets]\n",
    "            if len(sentiments) > 0:\n",
    "                score = sum(sentiments) / len(sentiments)\n",
    "                results.append((sentence, score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _export_target_sentences(doc, targets, export_path):\n",
    "    compiled_targets = [re.compile(target) for target in targets]\n",
    "    with open(export_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in doc.sents:\n",
    "            if any(target.search(sentence.lower_) for target in compiled_targets):\n",
    "                f.write(sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Die Nationale Akademie der Wissenschaften fordert zügige Hilfe für traumatisierte Flüchtlinge.,\n",
       "  0.004),\n",
       " (Die altehrwürdige Nationale Akademie der Wissenschaften fordert in einer Stellungnahme schnelle Hilfe für traumatisierte Flüchtlinge.,\n",
       "  0.060450000000000004),\n",
       " (Flüchtlinge hätten häufig immense Gewalt und lebensbedrohliche Situationen erlebt, warnen die Wissenschaftler.,\n",
       "  -0.5363),\n",
       " (Ein Teil der Flüchtlinge ist dadurch nicht in der Lage, den Alltag zu bewältigen, soziale Beziehungen einzugehen oder eine neue Sprache zu erlernen.\",\n",
       "  0.004),\n",
       " (Herr Bajbouj, welche Hilfen benötigen traumatisierte Flüchtlinge?, 0.004),\n",
       " (Sie fordern gemeinsam mit zwölf weiteren Autoren schnelle Hilfe für traumatisierte Flüchtlinge.,\n",
       "  0.041633333333333335),\n",
       " (Bislang fehlen systematische Untersuchungen, wie viele der Geflüchteten behandlungswürdige Erkrankungen haben.,\n",
       "  -0.5365),\n",
       " (Die könnten anderen Flüchtlingen beibringen, wie man mit Stress umgeht und ihn vermeidet.,\n",
       "  -0.0048),\n",
       " (Psychische Probleme einerseits, Flüchtlingsdasein andererseits - welche Gefahren ergeben sich, wenn beides zusammenfällt?,\n",
       "  -0.69325),\n",
       " (Es herrscht sowieso Therapeutenmangel, jetzt kommen noch Flüchtlinge hinzu - also muss man die Ausbildung hochfahren.,\n",
       "  0.004),\n",
       " (Wir können jetzt neue Behandlungsmodelle entwickeln, die auch außerhalb der Flüchtlingsgemeinschaft sinnvoll sind.,\n",
       "  0.004)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sentences(doc, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"Es müsste viel mehr passieren\"\\nEin Interview von Félice Gritti\\nDie Nationale Akademie der Wissensch',\n",
       "  -0.10117272727272733)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_articles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = os.path.join(os.getcwd(), \"input\", \"target_sentences.txt\")\n",
    "_export_target_sentences(doc, targets, export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- R. Remus, U. Quasthoff & G. Heyer: SentiWS - a Publicly Available German-language Resource for Sentiment Analysis.\n",
    "In: Proceedings of the 7th International Language Ressources and Evaluation (LREC'10), pp. 1168-1171, 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
